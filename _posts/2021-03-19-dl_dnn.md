---
title: "[Deep Learning] 심층 신경망(DNN) 실습"
category: Deep Learning
use_math: true
---

<br>

# 1. 심층 신경망(Deep Neural Network, DNN) 실습
> 3가지 문제를 예측하기 위해 Keras Modeling 실습을 진행함 

## 1) 이진 분류(Binary Classification)
> IMDB(Internet Movie DataBase) : 인터넷 영화 리뷰 데이터(텍스트)로부터 긍정/부정 분류 

### 자연어 처리 방법
![](/assets/images/posts/dl/imdb.png)

- Last Layer
  - The number of Node : 1
  - Activation : Sigmoid
  - Error Function: binary_crossentropy

### 실습
- <a href="https://colab.research.google.com/drive/1c9guEwKvuQI30G9gAjinkN4jTK6J4Hk7?usp=sharing">이진 분류 실습</a>

<br>

## 2) 단일 레이블 다중 분류(Categorical Classification)
> Handwritten Digits in the M(Mixed)-NIST Database : 손글씨 숫자 이미지로부터 10개(0 ~ 9) 숫자 분류<br> 
> Fashion Images in the M(Mixed)-NIST Database : 흑백 패션 이미지로부터 10개 카테고리 분류 

### 이미지 데이터 처리 방법

![](/assets/images/posts/dl/image_mnist.png)

**CNN을 사용하게 되면 처음에는 벡터로 바꾸지 않고 그대로 사용할 수 있지만, 이미지 분류를 위해 DNN으로 입력이 들어갈 때 Flatten()으로 펴주고 들어가게 된다.**

- Last Layer
  - The number of Node : the number of Class
  - Activation : Softmax
  - Error Function : Categorical_crossentropy

- 참고
  - 다중 레이블 다중 분류에서는 활성화함수를 Sigmoid를 사용하고, Cost function은 binary_crossentropy를 사용한다.

### 실습
- <a href="https://colab.research.google.com/drive/1zVWNnL35BYFHYFWP8pIRUw2vEIW3e3l1?usp=sharing">Digits MNIST 다중분류(Overfitting 미처리)</a>
- <a href="https://colab.research.google.com/drive/1tLsWjY9XBmIIeLbzePzXZtabbtZobqCz?usp=sharing">Digits MNIST 다중분류(Overfitting 처리)</a>
- <a href="https://colab.research.google.com/drive/1agHO6UiNlyBarvr2wa6_NMngs3ocrpWG?usp=sharing">Fashion MNIST 다중분류(Overfitting 처리</a>

<br>

## 3) 수치 예측(Regression Analysis)
> Boston Housing Price Dataset : 보스턴 집 값 예측

- Last Layer
  - The number of Node : 1
  - Activation : None
    - 수치 예측에서는 마지막 레이어에 Activation을 주면 안된다. 예를 들어, sigmoid를 주면 값이 0에서 1사이로 변경되기 때문에 실제 예측값을 알 수 없다.
    - 0과 1 사이 값에 대한 회귀일 경우에는 sigmoid 사용
  - Error Function : mse

### 실습
- <a href="https://colab.research.google.com/drive/1uWfvoWjiygaL9h_YKqT0Jkm4liatu6UU?usp=sharing">회귀 분석</a>

<br>
<br>

# 2. DNN에서의 Hyperparameter Optimization

- Layer & Node & Input_shape
- Activation function
- Loss & Optimizer
- Epoch & Batch_size
- Kernel_regularizer & Dropout
- K-fold Cross Validation
- Train vs. Validation Rate & Normalization & Standardization