---
title: "[Deep Learning] 순환 신경망(RNN)"
category: Deep Learning
use_math: true
---

<br>

## 1. 순서가 있는 데이터(순차적인 정보)

### 1) 시계열 데이터(Times Series)
- 앞 뒤 순서(상호 관계)가 존재하는 데이터
- 시간의 흐름에 영향을 받는 데이터
- 주기성(Periodicity), 추세(Trend), 계절성(Seasonality)가 존재한다.
- 학습 방법
  - 시간 순서에 따라 Train/Test Data를 분리한다.
  - 일정 기간의 $X$ 데이터로 $y$를 예측하도록 학습한다.
  - Hyper Parameter : 일정 기간 

### 2) 텍스트 및 음성 데이터(번역, 음성인식, 텍스트, 음악, 동영상)

<br>

## 2. 순환 신경망(Recurrent Neural Network, RNN)
> **순서가 있는 데이터**를 처리하기 위한 모델로서 '단기 기억'을 사용하는 신경망

<br>

![](/assets/images/posts/dl/rnn.jpg)

https://www.researchgate.net/figure/The-architecture-of-RNN_fig3_312593525

<br>

DNN과 CNN은 Layer 간 상태를 기억하지 않으며 입출력이 독립적으로 처리되고,

Layer 마다 독립적으로 가중치를 학습한다.

<br> 

RNN은 내부 **루프**가 존재하는 신경망으로 **1개의 Layer**에서 반복된다.

따라서 학습 단계에서 모든 Layer가 **같은 가중치를 공유**하여 사용한다.

<br>

**연속적인 데이터(Sequence Data)** 처리에 적합한 알고리즘으로, 

이전 단계의 정보인 **은닉층의 출력**이 계속 **순환**하면서 **입력값과 함께 학습에 사용**된다.

이 때 순환되는 은닉층의 출력을 **단기 기억(Short-Term Memory)**으로 부른다.

<br>

이를 수식으로 표현하면 아래와 같다.

$tanh \Big(W_{hx} \times X_t +(h_{t-1} \times W_{hh}) \Big)  -> h_t$

소괄호가 Short-Term Memory이다.

<br>

RNN은 바로 전 단계의 출력만 기억하므로 반복이 증가하면 처음 들어온 결과가 희미해지는 장기 의존성(Long-Term Dependency)이라는 문제가 발생한다.

Sequence가 깊지 않으면 크게 문제가 되지 않지만, Sequence가 깊어지게 될 경우 장기 의존성 문제가 발생한다. 

이러한 문제를 해결하기 위해 <a href="https://gilbertlim.github.io/deep%20learning/dl_lstm/">LSTM</a>이 등장하게 된다.

RNN은 다른 모델(CNN 등)과 결합하여 사용되기도 한다.

<br>

## 3. 적용 방법과 사례

### 1) 사례
- 이미지
  - 이미지, 비디오 캡셔닝(Image/Video Captioning)
  - 이미지, 비디오 생성
- 음성 & 음악
  - 음성 인식(Speech Recognition)
  - Conversation Modeling / Qustion Answering
  - 음악 생성
- 텍스트
  - 기계 번역(Machine Translation)
  - 텍스트 기반 감정 분류
  - 필기체 인식 및 합성
- 시계열 예측(Time Series Prediction)
  - 자전거 대여 횟수 예측

### 2) 적용 방법
> 무엇을 입력하고 출력하는지에 따라 유연하게 활용할 수 있음

![](/assets/images/posts/dl/rnn_example.png)

Fei-Fei Li & Andrej Karpathy & Justin Johnson Lecture

- Input : Red
- RNN : Green
- Output : Blue

#### (1) one to one
- 고양이 사진 -> 0 or 1

#### (2) one to many
- 이미지 캡셔닝(Image Captioning)
    - 고양이 사진 1 장 -> "고양이가 책상에 앉아 있어요"

#### (3) many to one
- 감정 분석(Sentimental Analytics)
    - "영화가 너무 재밌어요" -> 긍정 or 부정

#### (4) many to many
- 기계 번역(Machine Translation)
    - "영화가 너무 재밌어요" -> "The movie is so much fun."
- 동영상(= 여러개의 이미지) -> 설명 또는 번역

<br>

### RNN 모델링 관련 옵션
- `return_sequences = False` : 최종 Sequence의 출력만 내보냄, True일 경우 Sequence 마다 출력 발생
  - `Node=1`, `input_shape = (5, 1)`, `return_sequences = False`이면 출력 값은 `[[1.]]` 이다
  - `Node=1`, `input_shape = (5, 1)`, `return_sequences = True`이면 출력 값은 `[[1.], [1.], [1.], [1.], [1.]]` 이다
- RNN을 여러개 Layer로 Stack 시키는 경우
  - 전/후 Layer의 노드 개수를 동일하게 설정
  - 전 Layer는 `return_sequences = True`로 설정해야 한다.

  ```python
    model_4 = models.Sequential(name='Stackd_RNN')
    model_4.add(layers.SimpleRNN(3, # 노드 개수 똑같이
                                 input_shape=(5,1),
                                 return_sequences=True)) # 스택 사용 시 True
    model_4.add(layers.SimpleRNN(3, # 노드 개수 똑같이
                                 input_shape=(5,1),
                                 return_sequences=False))
    model_4.add(layers.Dense(1))
  
    model_4.compile(loss='mae', optimizer='adam', metrics=['accuracy'])
  
    model_4.predict(X_test[0].reshape(1,5,1))
    # 노드 개수가 4개일 때의 출력
    # array([[[-1., 1., 0.97619194, -1.],
    #         [-1., 1., 0.99765366, -1.],
    #         [-1., 1., 0.9796944,  -1.],
    #         [-1., 1., 0.99825597, -1.],
    #         [-1., 1., 0.998502,   -1.]]], dtype=float32)
  ```